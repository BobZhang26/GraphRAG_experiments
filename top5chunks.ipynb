{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Top 5 Chunks to Check Paper's Stance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv # type: ignore\n",
    "import os\n",
    "from langchain_neo4j import Neo4jGraph # type: ignore\n",
    "from libs import create_vector_index\n",
    "import pandas as pd # type: ignore\n",
    "from conn import connect2Googlesheet\n",
    "from annolibs import get_all_chunks_per_paper, compare_embeddings\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Neo4j database\n",
    "try:\n",
    "    graph = Neo4jGraph(\n",
    "        url=os.getenv(\"NEO4J_URL\"),\n",
    "        username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "        password=os.getenv(\"NEO4J_PASSWORD\")\n",
    "    )\n",
    "    print(\"Connected to Neo4j database successfully.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Could not connect to Neo4j database: {e}\")\n",
    "\n",
    "# Check if the entities index exists\n",
    "index_name = \"entities\"\n",
    "query = \"SHOW INDEXES YIELD name, type WHERE type = 'VECTOR' AND name = $index_name\"\n",
    "\n",
    "result = graph.query(query, params={\"index_name\": index_name})\n",
    "if result:\n",
    "    print(\"The 'entities' index already exists.\")\n",
    "else:\n",
    "    create_vector_index(graph, \"entities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Questions from Google Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet = connect2Googlesheet()\n",
    "\n",
    "# Select the worksheet: relevance\n",
    "worksheet = spreadsheet.get_worksheet(2)  \n",
    "\n",
    "# Get all records as a list of dictionaries\n",
    "data = worksheet.get_all_records()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df_Paper = pd.DataFrame(data)\n",
    "df_Paper.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Get the chunks from each paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "#papers = sorted(list(set([doc.strip() for doc in df_Paper['docs'].tolist()])))\n",
    "\n",
    "# Filter papers with 'Delirium' in condition\n",
    "delirium_papers = df_Paper[df_Paper['condition'].str.contains('Delirium', case=False, na=False)]\n",
    "\n",
    "# Display the filtered papers\n",
    "print(f\"Found {len(delirium_papers)} papers related to Delirium:\")\n",
    "display(delirium_papers)\n",
    "\n",
    "# Get unique paper names\n",
    "delirium_paper_names = delirium_papers['docs'].str.strip().unique()\n",
    "# print(f\"Number of unique paper names: {len(delirium_paper_names)}\")\n",
    "delirium_papers_chunks = get_all_chunks_per_paper(graph, delirium_paper_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the expected number of papers match the actual number of papers\n",
    "# Get list of expected papers\n",
    "expected_papers = delirium_paper_names.tolist()\n",
    "\n",
    "# Get list of actual papers from chunks_of_paper directory\n",
    "actual_papers = [f.replace('chunks_of_', '').replace('.csv', '') \n",
    "                 for f in os.listdir('./chunks_of_paper') \n",
    "                 if f.endswith('.csv')]\n",
    "\n",
    "# Find missing papers\n",
    "missing_papers = set(expected_papers) - set(actual_papers)\n",
    "\n",
    "print(\"Missing papers:\")\n",
    "for paper in missing_papers:\n",
    "    print(f\"- {paper}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Compare Question Embedding and Paper Chunk Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options to show the full text content\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "idx = 0\n",
    "test_question = df_Paper['Question'].iloc[idx]\n",
    "print(f\"Question {idx+1}: {test_question}\")\n",
    "\n",
    "# for i, paper in enumerate(delirium_paper_names):\n",
    "#     print(f\"\\nPaper {i+1}: {paper}\")\n",
    "#     paper_name = str(paper)  # without .pdf extension\n",
    "#     top5chunks = compare_embeddings(\n",
    "#         question=test_question,\n",
    "#         paper=paper_name,\n",
    "#         top_k=5\n",
    "#     )\n",
    "top5chunks = compare_embeddings(\n",
    "        question=test_question,\n",
    "        paper='AID-ICU',\n",
    "        top_k=5\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
